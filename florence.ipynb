{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0dc869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\Desktop\\Call-Audio-kush\\callenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM  \n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7cbad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\Desktop\\Call-Audio-kush\\callenv\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Florence-2-large:\n",
      "- processing_florence2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'microsoft/Florence-2-large'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype='auto').eval().cuda()\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe5a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(task_prompt, text_input=None):\n",
    "    if text_input is None:\n",
    "        prompt = task_prompt\n",
    "    else:\n",
    "        prompt = task_prompt + text_input\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to('cuda', torch.float16)\n",
    "    generated_ids = model.generate(\n",
    "      input_ids=inputs[\"input_ids\"].cuda(),\n",
    "      pixel_values=inputs[\"pixel_values\"].cuda(),\n",
    "      max_new_tokens=1024,\n",
    "      early_stopping=False,\n",
    "      do_sample=False,\n",
    "      num_beams=3,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text, \n",
    "        task=task_prompt, \n",
    "        image_size=(image.width, image.height)\n",
    "    )\n",
    "\n",
    "    return parsed_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f24942f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'better images/17.jpg'\n",
    "image = Image.open(path)\n",
    "# Cell 5: Perform OCR\n",
    "task_prompt = '<OCR>'\n",
    "ocr_result = run_example(task_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9a5bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_text=ocr_result['<OCR>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0fed754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extract the following key information from the OCR result below:\\n- Batch No. (or Lot No. or B No.)\\n- MFG Date (Manufacturing Date)\\n- Exp Date (Expiry Date)\\n- Price (M.R.P.)\\n\\nBETA\\nLETS\\nOLOL\\nBatch No. :\\nMfg. Date :\\nB7AFX011\\n07/2024\\nExpiry Date:\\n06/2026\\nM.R.P.₹\\n70.30\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_prompt = '''Extract the following key information from the OCR result below:\n",
    "- Batch No. (or Lot No. or B No.)\n",
    "- MFG Date (Manufacturing Date)\n",
    "- Exp Date (Expiry Date)\n",
    "- Price (M.R.P.)\n",
    "'''\n",
    "prompt = sys_prompt + '\\n' + ocr_text\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82a8d8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s the extracted information from the OCR result:\n",
      "\n",
      "*   **Batch No.:** B7AFX011\n",
      "*   **Mfg. Date:** 07/2024\n",
      "*   **Expiry Date:** 06/2026\n",
      "*   **M.R.P.₹:** 70.30\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def query_gemma(prompt, model=\"gemma3:1b\"):\n",
    "    payload = {\n",
    "        \"model\":model,\n",
    "        \"prompt\":prompt,\n",
    "        \"stream\":False\n",
    "    }\n",
    "    response = requests.post(OLLAMA_URL, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    prompt=prompt\n",
    "    result=query_gemma(prompt)\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "callenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
